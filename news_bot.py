"""
ê¸ˆìœµê¶Œ ë³´ì•ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í…”ë ˆê·¸ë¨ ì „ì†¡ ë´‡

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ APIì™€ Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬
ê¸ˆìœµê¶Œ ë³´ì•ˆ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³ , Groq APIë¡œ ì„ ë³„í•œ í›„
í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
"""

import os
import json
import html
import sqlite3
import requests
import re
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from difflib import SequenceMatcher
from tavily import TavilyClient

# ==========================================
# ë¡œê¹… ì„¤ì •
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ==========================================
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
# ==========================================
NAVER_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
TAVILY_KEY = os.environ.get("TAVILY_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

# ëŒ€í•œë¯¼êµ­ ì„œìš¸ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ ë‚ ì§œ
# Lambda ì›œ ì»¨í…Œì´ë„ˆ ìºì‹œ ë°©ì§€ë¥¼ ìœ„í•´ main()ì—ì„œ ì¬ê³„ì‚°
KST = ZoneInfo("Asia/Seoul")
_kst_now = datetime.now(KST)
NOW = _kst_now
TODAY_STR = _kst_now.strftime("%Y-%m-%d")
YESTERDAY = (_kst_now - timedelta(days=1)).strftime("%Y-%m-%d")


# ==========================================
# êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ (ë„¤ì´ë²„ API)
# ==========================================
def search_naver_news() -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ êµ­ë‚´ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    keywords = ["AIë³´ì•ˆ", "ì •ë³´ë³´í˜¸", "í•´í‚¹", "ê°œì¸ì •ë³´ìœ ì¶œ", "ê¸ˆìœµë³´ì•ˆ", "ëœì„¬ì›¨ì–´"]
    logger.info(f"ğŸ‡°ğŸ‡· [êµ­ë‚´] ë„¤ì´ë²„ ë¶„í•  ê²€ìƒ‰ ì‹œì‘: {keywords}")
    
    if not NAVER_ID or not NAVER_SECRET:
        logger.error("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_ID,
        "X-Naver-Client-Secret": NAVER_SECRET
    }
    
    all_collected = {}  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬

    for keyword in keywords:
        try:
            params = {"query": keyword, "display": 15, "sort": "date"}
            res = requests.get(url, headers=headers, params=params, timeout=10)
            
            if res.status_code == 200:
                items = res.json().get('items', [])
                for item in items:
                    try:
                        # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
                        pub_date_str = item.get('pubDate', '')
                        if pub_date_str:
                            pub_dt = datetime.strptime(
                                pub_date_str,
                                "%a, %d %b %Y %H:%M:%S +0900"
                            )
                            pub_date_fmt = pub_dt.strftime("%Y-%m-%d")
                            if pub_date_fmt < YESTERDAY:
                                continue
                        else:
                            pub_date_fmt = TODAY_STR
                    except Exception as e:
                        logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                        pub_date_fmt = TODAY_STR

                    # ì¤‘ë³µ ì²´í¬
                    link = item.get('originallink') or item.get('link', '')
                    if not link or link in all_collected:
                        continue

                    # HTML íƒœê·¸ ì œê±° ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
                    clean_title = re.sub('<.+?>', '', item.get('title', ''))
                    clean_title = html.unescape(clean_title)

                    clean_desc = re.sub('<.+?>', '', item.get('description', ''))
                    clean_desc = html.unescape(clean_desc)

                    all_collected[link] = {
                        "category": "[êµ­ë‚´]",
                        "title": clean_title,
                        "url": link,
                        "published_date": pub_date_fmt,
                        "description": clean_desc
                    }
            else:
                logger.warning(f"ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨ (í‚¤ì›Œë“œ: {keyword}): {res.status_code}")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"ë„¤ì´ë²„ API ìš”ì²­ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
            
    final_list = list(all_collected.values())
    logger.info(f"   ğŸ‘‰ êµ­ë‚´ í›„ë³´ ì´ {len(final_list)}ê±´ í™•ë³´")
    
    # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    def calculate_priority_score(article: Dict[str, str]) -> int:
        """ê¸°ì‚¬ì˜ ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        score = 0
        title = article['title'].lower()
        desc = article['description'].lower()
        
        # 1ìˆœìœ„ í‚¤ì›Œë“œ (AIë³´ì•ˆ, ì¹¨í•´ì‚¬ê³ ) - 10ì 
        high_priority = ['aië³´ì•ˆ', 'í•´í‚¹', 'ìœ ì¶œ', 'ëœì„¬ì›¨ì–´', 'ì‚¬ì´ë²„ê³µê²©', 'ë³´ì•ˆì‚¬ê³ ', 'ì¹¨í•´']
        score += sum(10 for k in high_priority if k in title or k in desc)
        
        # 2ìˆœìœ„ í‚¤ì›Œë“œ (ì œë„/ê¸°ìˆ ) - 5ì 
        mid_priority = ['ê¸ˆìœµë³´ì•ˆì›', 'ê¸ˆê°ì›', 'ê·œì œ', 'ë³´ì•ˆê¸°ìˆ ', 'ì œë¡œë°ì´', 'ì·¨ì•½ì ']
        score += sum(5 for k in mid_priority if k in title or k in desc)
        
        # 3ìˆœìœ„ í‚¤ì›Œë“œ (ì‹ í•œ) - 3ì 
        if 'ì‹ í•œ' in title or 'ì‹ í•œ' in desc:
            score += 3
        
        # ë‚ ì§œ ê°€ì¤‘ì¹˜ (ë‹¹ì¼ ê¸°ì‚¬ ìš°ëŒ€) - 2ì 
        if article['published_date'] == TODAY_STR:
            score += 2
        
        return score
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 20ê°œë§Œ ì„ íƒ
    final_list.sort(key=calculate_priority_score, reverse=True)
    if len(final_list) > 20:
        final_list = final_list[:20]
        logger.info(f"   âœ‚ï¸ ìƒìœ„ 20ê°œë¡œ ì••ì¶• (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì„ ë³„)")
        
    return final_list


# ==========================================
# í•´ì™¸ ë‰´ìŠ¤ ê²€ìƒ‰ (Tavily API)
# ==========================================
def search_tavily_news() -> List[Dict[str, str]]:
    """
    Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ì™¸ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸ‡ºğŸ‡¸ [í•´ì™¸] Tavily ê²€ìƒ‰ ì‹œì‘...")
    
    if not TAVILY_KEY:
        logger.error("âŒ Tavily API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    try:
        tavily = TavilyClient(api_key=TAVILY_KEY)
        
        domains = [
            "thehackernews.com",
            "bleepingcomputer.com",
            "darkreading.com",
            "securityweek.com",
            "wired.com",
            "techcrunch.com"
        ]
        
        res = tavily.search(
            query="Cyber Security Breach Hacking News",
            topic="news",
            days=2,
            include_domains=domains,
            max_results=40
        )
        
        collected = []
        for item in res.get('results', []):
            pub_date = item.get('published_date', '')
            if pub_date is None:
                pub_date = ""
            
            # ë‚ ì§œ í•„í„°ë§ (í˜„ì¬ ì—°ë„ê°€ ì•„ë‹ˆê±°ë‚˜ 'ago'ê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸)
            current_year = str(NOW.year)
            if pub_date and (current_year not in pub_date and 'ago' not in pub_date):
                continue

            collected.append({
                "category": "[í•´ì™¸]",
                "title": item.get('title', ''),
                "url": item.get('url', ''),
                "published_date": pub_date,
                "description": item.get('content', '')[:200]
            })
        
        # ìƒìœ„ 20ê°œë¡œ ì œí•œ
        collected = collected[:20]
        logger.info(f"   ğŸ‘‰ í•´ì™¸ í›„ë³´ {len(collected)}ê°œ í™•ë³´ (í•„í„°ë§ ì™„ë£Œ)")
        return collected
        
    except Exception as e:
        logger.error(f"âŒ Tavily ì˜¤ë¥˜: {e}")
        return []


# ==========================================
# ë¡œì»¬ í•„í„°ë§ (API ì‚¬ìš©ëŸ‰ ì ˆê°)
# ==========================================
def simple_rule_filter(articles: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ëª…ë°±í•œ ì œì™¸ ëŒ€ìƒì„ ë¡œì»¬ì—ì„œ í•„í„°ë§í•˜ì—¬ API ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
    
    Args:
        articles: í•„í„°ë§í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Dict]: í•„í„°ë§ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []
    
    filtered = []
    exclude_keywords = ['ì±„ìš©', 'ì¸ì‚¬ë°œë ¹', 'ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜', 'ê´‘ê³ ', 'ëª¨ì§‘']
    
    for article in articles:
        title = article.get('title', '').lower()
        desc = article.get('description', '').lower()
        
        # ëª…ë°±íˆ ê´€ë ¨ ì—†ëŠ” ê²ƒë§Œ ì œì™¸
        if any(kw in title or kw in desc for kw in exclude_keywords):
            continue
        
        filtered.append(article)
    
    logger.info(f"   ğŸ” ë¡œì»¬ í•„í„°ë§: {len(articles)}ê°œ â†’ {len(filtered)}ê°œ")
    return filtered


def remove_duplicate_articles(articles: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ì œëª© ìœ ì‚¬ë„ + í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    ê°™ì€ ì‚¬ê±´ì„ ë‹¤ë£¬ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
    
    Args:
        articles: ì¤‘ë³µ ì œê±°í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Dict]: ì¤‘ë³µì´ ì œê±°ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []
    
    def extract_keywords(title: str) -> set:
        """ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (íšŒì‚¬ëª…, ì‚¬ê±´ëª… ë“±)"""
        # ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', title)
        # 3ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ í‚¤ì›Œë“œë¡œ ê°„ì£¼
        keywords = {w.lower() for w in words if len(w) >= 3}
        return keywords
    
    unique = []
    keywords_cache = []  # ê¸°ì¡´ ê¸°ì‚¬ í‚¤ì›Œë“œ ìºì‹œ (uniqueì™€ ë™ì¼ ì¸ë±ìŠ¤)

    for article in articles:
        title = article.get('title', '')
        is_duplicate = False

        # í˜„ì¬ ê¸°ì‚¬ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        current_keywords = extract_keywords(title)

        # ê¸°ì¡´ unique ë¦¬ìŠ¤íŠ¸ì˜ ê¸°ì‚¬ë“¤ê³¼ ë¹„êµ
        for i, existing in enumerate(unique):
            existing_title = existing['title']
            existing_keywords = keywords_cache[i]

            # 1. ì œëª© ìœ ì‚¬ë„ ì²´í¬ (60% ì´ìƒ â†’ ì¤‘ë³µ)
            similarity = SequenceMatcher(
                None,
                title.lower(),
                existing_title.lower()
            ).ratio()

            # 2. í‚¤ì›Œë“œ ì¤‘ë³µë¥  ì²´í¬ (ê³µí†µ í‚¤ì›Œë“œê°€ 50% ì´ìƒ â†’ ì¤‘ë³µ)
            if current_keywords and existing_keywords:
                common_keywords = current_keywords & existing_keywords
                keyword_overlap = len(common_keywords) / min(len(current_keywords), len(existing_keywords))
            else:
                keyword_overlap = 0

            # ìœ ì‚¬ë„ 60% ì´ìƒ OR í‚¤ì›Œë“œ ì¤‘ë³µ 50% ì´ìƒ â†’ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            if similarity > 0.60 or keyword_overlap > 0.50:
                is_duplicate = True
                # ë” ê¸´ ì œëª©(ë” ìƒì„¸í•œ ê¸°ì‚¬)ì„ ì„ íƒ
                if len(title) > len(existing_title):
                    unique[i] = article
                    keywords_cache[i] = current_keywords
                    logger.debug(f"   ğŸ”„ ì¤‘ë³µ êµì²´ (ìœ ì‚¬ë„:{similarity:.0%}, í‚¤ì›Œë“œ:{keyword_overlap:.0%})")
                    logger.debug(f"      '{existing_title[:30]}...' â†’ '{title[:30]}...'")
                break

        if not is_duplicate:
            unique.append(article)
            keywords_cache.append(current_keywords)
    
    removed_count = len(articles) - len(unique)
    if removed_count > 0:
        logger.info(f"   ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {len(articles)}ê°œ â†’ {len(unique)}ê°œ ({removed_count}ê°œ ì œê±°)")
    else:
        logger.info(f"   âœ… ì¤‘ë³µ ì—†ìŒ: {len(articles)}ê°œ ìœ ì§€")
    
    return unique


# ==========================================
# AI ì„ ë³„ (Groq API) - ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹
# ==========================================
def call_groq_batch_selection(
    items: List[Dict[str, str]]
) -> List[Dict[str, str]]:
    """
    Groq API (Llama 3.3 70B)ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ­ë‚´Â·í•´ì™¸ ë‰´ìŠ¤ë¥¼ í•œ ë²ˆì— ì„ ë³„í•©ë‹ˆë‹¤.
    (API í˜¸ì¶œ 2íšŒ â†’ 1íšŒë¡œ ì ˆê°)

    Args:
        items: ì„ ë³„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (êµ­ë‚´ + í•´ì™¸)

    Returns:
        List[Dict]: ì„ ë³„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not items:
        return []

    if not GROQ_API_KEY:
        logger.error("âŒ Groq API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        'Authorization': f'Bearer {GROQ_API_KEY}',
        'Content-Type': 'application/json'
    }
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í•  ì •ì˜)
    system_prompt = """ë„ˆëŠ” ê¸ˆìœµê¶Œ ë³´ì•ˆ ë‰´ìŠ¤ ì „ë¬¸ íë ˆì´í„°ë‹¤.
ë‰´ìŠ¤ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ ë³„í•œë‹¤.

ìš°ì„ ìˆœìœ„:
1. AIë³´ì•ˆ (AI ë³´ì•ˆ ê´€ë ¨ ë‰´ìŠ¤) - ìµœìš°ì„ 
2. ì¹¨í•´ì‚¬ê³  (í•´í‚¹/ìœ ì¶œ/ëœì„¬ì›¨ì–´/ì‚¬ì´ë²„ê³µê²©) - ìµœìš°ì„ 
3. ê·œì œ/ì •ì±… (ê¸ˆìœµë‹¹êµ­Â·ë³´ì•ˆì› ë°œí‘œ, ë²•ê·œ ê°œì •)
4. ê¸°ìˆ /ì·¨ì•½ì  (ì œë¡œë°ì´, ìƒˆ ê³µê²©ê¸°ë²•)
5. ì‹ í•œ ê´€ë ¨ (+ê°€ì )

ì œì™¸: í™ë³´ì„±, ë‹¨ìˆœ ì¸ì‚¬, ì¤‘ë³µ ë‚´ìš©"""
    
    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì¤‘ë³µ ì œê±° ê·œì¹™ ê°•í™”)
    user_prompt = f"""ì•„ë˜ ê¸°ì‚¬ ì¤‘ì—ì„œ:
- [êµ­ë‚´] íƒœê·¸ ê¸°ì‚¬ ì¤‘ ìƒìœ„ 7ê°œ
- [í•´ì™¸] íƒœê·¸ ê¸°ì‚¬ ì¤‘ ìƒìœ„ 3ê°œ
ì´ 10ê°œë¥¼ ì„ ë³„í•´ë¼.

âš ï¸ **ì¤‘ë³µ ì œê±° ê·œì¹™ (ë§¤ìš° ì¤‘ìš”)**:
1. ê°™ì€ ì‚¬ê±´/ì‚¬ê³ ë¥¼ ë‹¤ë£¬ ê¸°ì‚¬ëŠ” **ë°˜ë“œì‹œ 1ê°œë§Œ** ì„ íƒ
2. ì œëª©ì´ ë¹„ìŠ·í•œ ê¸°ì‚¬ë“¤ ì¤‘ **ê°€ì¥ ìƒì„¸í•œ 1ê°œ**ë§Œ ì„ íƒ
3. ì˜ˆì‹œ:
   âœ… "SKì‰´ë”ìŠ¤, ì¶©ì „ê¸° í•´í‚¹ ì„±ê³µ" (ì„ íƒ)
   âŒ "SKì‰´ë”ìŠ¤, í°íˆ¬ì˜¨ì„œ ì¶©ì „ê¸° í•´í‚¹" (ìœ„ì™€ ì¤‘ë³µ, ì œì™¸)
   âŒ "ì „ê¸°ì°¨ ì¶©ì „ê¸° í•´í‚¹... SKì‰´ë”ìŠ¤" (ìœ„ì™€ ì¤‘ë³µ, ì œì™¸)
4. ë‹¤ì–‘í•œ ì‚¬ê±´ì„ ë‹¤ë£¬ ê¸°ì‚¬ë¥¼ ì„ íƒ (í•œ ì‚¬ê±´ì— 5ê°œ X)

âš ï¸ **í•´ì™¸ ê¸°ì‚¬ í•„ìˆ˜**:
- [í•´ì™¸] íƒœê·¸ ê¸°ì‚¬ë¥¼ **ë°˜ë“œì‹œ ì°¾ì•„ì„œ** 3ê°œ ì„ íƒ
- [í•´ì™¸] ê¸°ì‚¬ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ìˆëŠ” ë§Œí¼ë§Œ í¬í•¨
- [í•´ì™¸] ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ êµ­ë‚´ ê¸°ì‚¬ë¡œë§Œ 10ê°œ êµ¬ì„±

[ì…ë ¥ ë°ì´í„°]
{json.dumps(items, ensure_ascii=False, indent=2)}

[ì¶œë ¥ í¬ë§·]
JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥:
[
  {{
    "category": "[êµ­ë‚´ or í•´ì™¸]",
    "title": "ì œëª© (í•´ì™¸ ê¸°ì‚¬ëŠ” í•œê¸€ë¡œ ë²ˆì—­)",
    "title_original": "ì›ë¬¸ ì œëª© (í•´ì™¸ ê¸°ì‚¬ë§Œ, êµ­ë‚´ëŠ” ìƒëµ)",
    "url": "ë§í¬",
    "detected_date": "YYYY-MM-DD",
    "summary": "150ì ì´ë‚´ 3ì¤„ ìš”ì•½ (1ì¤„: ì‚¬ê±´ìš”ì•½, 2ì¤„: ì¤‘ìš”í•œ ì´ìœ , 3ì¤„: ì‹œì‚¬ì /ì „ë§)"
  }}
]

âš ï¸ **summary ê·œì¹™**:
- summary: ê¸°ì‚¬ í•µì‹¬ì„ 150ì ì´ë‚´, 3ì¤„ë¡œ ìš”ì•½. ê° ì¤„ì€ í•µì‹¬ ì‚¬ì‹¤ í•˜ë‚˜ì”© ë‹´ì„ ê²ƒ
- 1ì¤„: ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ê°€ (ì‚¬ê±´/ë°œí‘œ ìš”ì•½)
- 2ì¤„: ì™œ ì¤‘ìš”í•œê°€ (ì˜í–¥/ë°°ê²½)
- 3ì¤„: ì–´ë–¤ ì˜ë¯¸ê°€ ìˆëŠ”ê°€ (ì‹œì‚¬ì /ì „ë§)

âš ï¸ **í•´ì™¸ ê¸°ì‚¬ ë²ˆì—­ ê·œì¹™**:
- [í•´ì™¸] ê¸°ì‚¬ì˜ titleì€ **ë°˜ë“œì‹œ í•œê¸€ë¡œ ë²ˆì—­**
- title_originalì— ì˜ì–´ ì›ë¬¸ ë³´ê´€
- ë²ˆì—­ì€ ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ (ì§ì—­X, ì˜ì—­O)
- êµ­ë‚´ ê¸°ì‚¬ëŠ” title_original í•„ë“œ ìƒëµ"""
    
    # OpenAI API ìš”ì²­
    data = {
        "model": "llama-3.3-70b-versatile",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.1,
        "max_tokens": 4096
    }
    
    # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
    for attempt in range(3):
        try:
            res = requests.post(url, headers=headers, json=data, timeout=60)
            
            if res.status_code == 200:
                response_data = res.json()
                
                # OpenAI ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    content = response_data['choices'][0]['message']['content']
                    clean_text = content.replace("```json", "").replace("```", "").strip()
                    
                    try:
                        # JSON íŒŒì‹± (ë°°ì—´ ë˜ëŠ” ê°ì²´)
                        parsed = json.loads(clean_text)

                        # ë°°ì—´ì´ ì•„ë‹ˆë¼ ê°ì²´ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬
                        if isinstance(parsed, dict):
                            # ëª¨ë“  í‚¤ë¥¼ ê²€ì‚¬í•˜ì—¬ ë°°ì—´ ì°¾ê¸°
                            result = []
                            for key, value in parsed.items():
                                if isinstance(value, list) and len(value) > 0:
                                    result = value
                                    logger.info(f"   ğŸ“‹ JSON í‚¤ '{key}'ì—ì„œ {len(value)}ê°œ í•­ëª© ë°œê²¬")
                                    break

                            if not result:
                                logger.warning(f"   âš ï¸ JSON ê°ì²´ì—ì„œ ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                logger.warning(f"   ğŸ“„ ì‘ë‹µ í‚¤: {list(parsed.keys())}")
                        else:
                            result = parsed
                        
                        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                        required_fields = {'title', 'url', 'category'}
                        result = [
                            item for item in result
                            if isinstance(item, dict) and required_fields.issubset(item.keys())
                        ]

                        if result:
                            # êµ­ë‚´ â†’ í•´ì™¸ ìˆœì„œë¡œ ì •ë ¬
                            domestic = [a for a in result if '[êµ­ë‚´]' in a.get('category', '')]
                            overseas = [a for a in result if '[í•´ì™¸]' in a.get('category', '')]
                            result = domestic + overseas

                            overseas_count = len(overseas)
                            domestic_count = len(domestic)
                            
                            # í•´ì™¸ ê¸°ì‚¬ ë¶€ì¡± ì‹œ ê²½ê³ 
                            if overseas_count == 0:
                                logger.warning("   âš ï¸ í•´ì™¸ ê¸°ì‚¬ê°€ ì„ ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                logger.warning("   ğŸ’¡ Tavily ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ ë˜ëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°ì • í•„ìš”")
                            elif overseas_count < 3:
                                logger.warning(f"   âš ï¸ í•´ì™¸ ê¸°ì‚¬ {overseas_count}ê°œë§Œ ì„ ë³„ë¨ (ëª©í‘œ: 3ê°œ)")
                            
                            logger.info(f"   âœ… AI ë°°ì¹˜ ì„ ë³„ ì™„ë£Œ (Groq): {len(result)}ê°œ (êµ­ë‚´ {domestic_count}, í•´ì™¸ {overseas_count})")
                            return result
                        else:
                            logger.warning(f"   âš ï¸ ì„ ë³„ëœ ê¸°ì‚¬ê°€ ì—†ìŒ")
                    except json.JSONDecodeError as e:
                        logger.warning(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        logger.debug(f"   ì‘ë‹µ ë‚´ìš©: {clean_text[:200]}")
                        if attempt < 2:
                            continue
                        return []
                else:
                    logger.warning(f"   âš ï¸ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
                    return []
                    
            elif res.status_code == 429:
                wait_time = (attempt + 1) * 10
                logger.warning(f"   â³ [API ê³¼ë¶€í•˜] {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
                continue
            else:
                logger.error(f"   âŒ API ì˜¤ë¥˜: {res.status_code} - {res.text[:200]}")
                if res.status_code >= 500:
                    time.sleep(10)
                    continue
                return []
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"   âš ï¸ [ì—°ê²° ë¶ˆì•ˆì •] {e}. 10ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/3)...")
            time.sleep(10)
            continue
        except Exception as e:
            logger.error(f"   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            if attempt < 2:
                time.sleep(10)
                continue
            return []
            
    logger.error(f"   âŒ 3íšŒ ì¬ì‹œë„ ì‹¤íŒ¨")
    return []


# ==========================================
# ë‰´ìŠ¤ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)
# ==========================================
def process_news() -> List[Dict[str, str]]:
    """
    êµ­ë‚´ ë° í•´ì™¸ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  AIë¡œ ì„ ë³„í•©ë‹ˆë‹¤.
    (ë°°ì¹˜ ì²˜ë¦¬: API í˜¸ì¶œ 2íšŒ â†’ 1íšŒë¡œ ì ˆê°)
    
    Returns:
        List[Dict]: ìµœì¢… ì„ ë³„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # 1. êµ­ë‚´ + í•´ì™¸ ë‰´ìŠ¤ ë³‘ë ¬ ìˆ˜ì§‘
        logger.info("\nğŸ“° [1ë‹¨ê³„] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        with ThreadPoolExecutor(max_workers=2) as executor:
            kr_future = executor.submit(search_naver_news)
            en_future = executor.submit(search_tavily_news)
            kr_candidates = kr_future.result()
            en_candidates = en_future.result()
        
        # 3. ë¡œì»¬ í•„í„°ë§ (ëª…ë°±í•œ ì œì™¸ ëŒ€ìƒ ì‚¬ì „ ì œê±°)
        logger.info("\nğŸ” [2ë‹¨ê³„] ë¡œì»¬ í•„í„°ë§ ì¤‘...")
        if kr_candidates:
            kr_filtered = simple_rule_filter(kr_candidates)
        else:
            kr_filtered = []
            logger.warning("   âš ï¸ êµ­ë‚´ í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if en_candidates:
            en_filtered = simple_rule_filter(en_candidates)
        else:
            en_filtered = []
            logger.warning("   âš ï¸ í•´ì™¸ í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 4. ì¤‘ë³µ ì œê±° (ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜)
        logger.info("\nğŸ—‘ï¸ [2.5ë‹¨ê³„] ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")
        all_candidates = kr_filtered + en_filtered
        
        if not all_candidates:
            logger.warning("âš ï¸ í•„í„°ë§ í›„ í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # êµ­ë‚´, í•´ì™¸ ê°ê° ì¤‘ë³µ ì œê±° í›„ í•©ì¹˜ê¸°
        kr_unique = remove_duplicate_articles(kr_filtered) if kr_filtered else []
        en_unique = remove_duplicate_articles(en_filtered) if en_filtered else []
        all_candidates = kr_unique + en_unique
        
        logger.info(f"   ğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(all_candidates)}ê°œ (êµ­ë‚´ {len(kr_unique)} + í•´ì™¸ {len(en_unique)})")
        
        if not all_candidates:
            logger.warning("âš ï¸ ì¤‘ë³µ ì œê±° í›„ í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        logger.info(f"\nğŸ¤– [3ë‹¨ê³„] AIê°€ êµ­ë‚´ 7ê°œ + í•´ì™¸ 3ê°œë¥¼ ì„ ë³„í•©ë‹ˆë‹¤...")
        logger.info(f"   ğŸ’¡ ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ 1íšŒë§Œ ì‚¬ìš© (Groq Llama-3.3-70B)")

        final_list = call_groq_batch_selection(all_candidates)
        
        if final_list:
            logger.info(f"   âœ… ìµœì¢… {len(final_list)}ê°œ ì„ ë³„ ì™„ë£Œ")
        else:
            logger.warning("   âš ï¸ AI ì„ ë³„ ì‹¤íŒ¨")
        
        return final_list
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


# ==========================================
# ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (GPT-4o)
# ==========================================
def generate_strategic_analysis(articles: List[Dict[str, str]]) -> str:
    """
    ì„ ë³„ëœ ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ CISO/ì •ë³´ë³´í˜¸íŒ€ì¥ ìˆ˜ì¤€ì˜ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        articles: ì„ ë³„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (10ê°œ)

    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸
    """
    if not articles:
        return ""

    if not OPENAI_API_KEY:
        logger.error("âŒ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    logger.info("\nğŸ“ [ì „ëµì  ë¶„ì„] GPT-4oë¡œ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

    # ê¸°ì‚¬ ìš”ì•½ ëª©ë¡ êµ¬ì„±
    article_list = ""
    for i, art in enumerate(articles, 1):
        category = art.get('category', '')
        title = art.get('title', '')
        summary = art.get('summary', '')
        article_list += f"[{i}] {category} {title}\n    ìš”ì•½: {summary}\n"

    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        'Authorization': f'Bearer {OPENAI_API_KEY}',
        'Content-Type': 'application/json'
    }

    system_prompt = """ë„ˆëŠ” ê¸ˆìœµê¶Œ CISO ìë¬¸ì—­ì´ë‹¤.
ë§¤ì¼ ì„ ë³„ëœ ë³´ì•ˆ ë‰´ìŠ¤ 10ê±´ì„ ì¢…í•© ë¶„ì„í•˜ì—¬, ê¸ˆìœµì‚¬ ì •ë³´ë³´í˜¸íŒ€ì¥ì´ ê²½ì˜ì§„ì—ê²Œ ë³´ê³ í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ ì „ëµì  ë¸Œë¦¬í•‘ì„ ì‘ì„±í•œë‹¤.

ì‘ì„± ì›ì¹™:
- ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ ë§¥ë½ê³¼ ì˜ë¯¸ í•´ì„
- ê¸ˆìœµê¶Œ íŠ¹ìˆ˜ì„±(ê·œì œ, ê³ ê°ë°ì´í„°, ì‹ ë¢°)ì„ ë°˜ì˜
- ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ [N] í˜•ì‹ìœ¼ë¡œ ì°¸ì¡°
- í•œê¸€ ê¸°ì¤€ 1,500~3,000ì"""

    user_prompt = f"""ì•„ë˜ 10ê°œ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ 3íŒŒíŠ¸ ì „ëµì  ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.

[ê¸°ì‚¬ ëª©ë¡]
{article_list}

[ì¶œë ¥ í˜•ì‹ - ë§ˆí¬ë‹¤ìš´]

## 1. ìš”ì•½: (í•µì‹¬ í…Œë§ˆë¥¼ í¬ê´„í•˜ëŠ” ì†Œì œëª©)

ë‹¹ì¼ ê¸°ì‚¬ë¥¼ 2~3ê°œ í•µì‹¬ í…Œë§ˆë¡œ ë¬¶ì–´ ë¶„ì„.
ê° í…Œë§ˆì— ì†Œì œëª©ì„ ë¶€ì—¬í•˜ê³ , ê´€ë ¨ ê¸°ì‚¬ë¥¼ [ë²ˆí˜¸]ë¡œ ì°¸ì¡°.

### A. (í…Œë§ˆ ì†Œì œëª©)
ë¶„ì„ ë‚´ìš©... [N][M]

### B. (í…Œë§ˆ ì†Œì œëª©)
ë¶„ì„ ë‚´ìš©... [N]

## 2. ê¸ˆìœµì‚¬ ì •ë³´ë³´í˜¸íŒ€ì„ ìœ„í•œ ì „ëµì  ì œì–¸

ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ 3ê°œ ë‚´ì™¸ ì•¡ì…˜ ì•„ì´í…œ. ê°ê° Logicê³¼ Action í¬í•¨.

### â‘  (ì œì–¸ ì œëª©)
- Logic: ...
- Action: ...

### â‘¡ (ì œì–¸ ì œëª©)
- Logic: ...
- Action: ...

## 3. ìƒê°í•´ë³¼ ì§ˆë¬¸

ì •ë³´ë³´í˜¸íŒ€ ë‚´ í† ë¡ ìš© ë„ë°œì  ì§ˆë¬¸ 2~3ê°œ. ë‹¹ì¼ ê¸°ì‚¬ì™€ ì—°ê²°í•˜ë˜ ìì‚¬ ì ìš© ê´€ì .

### Q1
ì§ˆë¬¸ ë‚´ìš©

### Q2
ì§ˆë¬¸ ë‚´ìš©"""

    data = {
        "model": "gpt-4o",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.4,
        "max_tokens": 4000
    }

    for attempt in range(3):
        try:
            res = requests.post(url, headers=headers, json=data, timeout=90)

            if res.status_code == 200:
                response_data = res.json()
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    content = response_data['choices'][0]['message']['content']
                    logger.info(f"   âœ… ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ({len(content)}ì)")
                    return content
                else:
                    logger.warning("   âš ï¸ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
                    return ""
            elif res.status_code == 429:
                wait_time = (attempt + 1) * 15
                logger.warning(f"   â³ [API ê³¼ë¶€í•˜] {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
                continue
            else:
                logger.error(f"   âŒ API ì˜¤ë¥˜: {res.status_code} - {res.text[:200]}")
                if res.status_code >= 500:
                    time.sleep(10)
                    continue
                return ""

        except requests.exceptions.RequestException as e:
            logger.warning(f"   âš ï¸ [ì—°ê²° ë¶ˆì•ˆì •] {e}. 10ì´ˆ í›„ ì¬ì‹œë„ ({attempt+1}/3)...")
            time.sleep(10)
            continue
        except Exception as e:
            logger.error(f"   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            if attempt < 2:
                time.sleep(10)
                continue
            return ""

    logger.error("   âŒ ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± 3íšŒ ì¬ì‹œë„ ì‹¤íŒ¨")
    return ""


# ==========================================
# SQLite ì €ì¥
# ==========================================
def save_to_sqlite(
    articles: List[Dict[str, str]],
    analysis: str,
    date_str: str
) -> bool:
    """
    ì„ ë³„ëœ ê¸°ì‚¬ì™€ ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ SQLite DBì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        articles: ì„ ë³„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        analysis: ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ (ë§ˆí¬ë‹¤ìš´)
        date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)

    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    if not articles:
        logger.warning("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    db_path = Path(__file__).parent / "web" / "data" / "news.db"
    db_path.parent.mkdir(parents=True, exist_ok=True)

    logger.info(f"\nğŸ’¾ [SQLite] {db_path} ì— ì €ì¥ ì¤‘...")

    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()

        # í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS daily_briefings (
                date        TEXT PRIMARY KEY,
                analysis    TEXT NOT NULL,
                created_at  TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS articles (
                id              INTEGER PRIMARY KEY AUTOINCREMENT,
                date            TEXT NOT NULL,
                category        TEXT,
                title           TEXT NOT NULL,
                title_original  TEXT,
                url             TEXT NOT NULL,
                summary         TEXT,
                insight         TEXT,
                detected_date   TEXT,
                created_at      TEXT NOT NULL
            )
        """)

        now_iso = datetime.now(KST).isoformat()

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°™ì€ ë‚ ì§œ ì¤‘ë³µ ë°©ì§€)
        cursor.execute("DELETE FROM daily_briefings WHERE date = ?", (date_str,))
        cursor.execute("DELETE FROM articles WHERE date = ?", (date_str,))

        # ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
        if analysis:
            cursor.execute(
                "INSERT INTO daily_briefings (date, analysis, created_at) VALUES (?, ?, ?)",
                (date_str, analysis, now_iso)
            )

        # ê¸°ì‚¬ ì €ì¥
        for art in articles:
            cursor.execute(
                """INSERT INTO articles
                   (date, category, title, title_original, url, summary, insight, detected_date, created_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (
                    date_str,
                    art.get('category', ''),
                    art.get('title', ''),
                    art.get('title_original', ''),
                    art.get('url', ''),
                    art.get('summary', ''),
                    '',
                    art.get('detected_date', ''),
                    now_iso
                )
            )

        conn.commit()
        conn.close()
        logger.info(f"   âœ… SQLite ì €ì¥ ì™„ë£Œ: ê¸°ì‚¬ {len(articles)}ê±´, ë¶„ì„ ë¦¬í¬íŠ¸ 1ê±´")
        return True

    except Exception as e:
        logger.error(f"   âŒ SQLite ì €ì¥ ì˜¤ë¥˜: {e}")
        return False


# ==========================================
# í…”ë ˆê·¸ë¨ ì „ì†¡
# ==========================================
def send_telegram(articles: List[Dict[str, str]]) -> bool:
    """
    ì„ ë³„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        articles: ì „ì†¡í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        bool: ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    if not articles:
        logger.warning("âš ï¸ ì „ì†¡í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logger.warning("âš ï¸ í…”ë ˆê·¸ë¨ ì„¤ì •ì´ ì—†ì–´ í…”ë ˆê·¸ë¨ ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False

    # ì±„íŒ… ID ê²€ì¦ ë° ë³€í™˜ (ìˆ«ì ë¬¸ìì—´ë¡œ ë³€í™˜)
    try:
        chat_id = str(TELEGRAM_CHAT_ID).strip()
        # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
        int(chat_id)
    except ValueError:
        logger.error(f"âŒ í…”ë ˆê·¸ë¨ ì±„íŒ… IDê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {TELEGRAM_CHAT_ID}")
        logger.error("   ğŸ’¡ ì±„íŒ… IDëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤. ê°œì¸ ì±„íŒ…ì˜ ê²½ìš° ë´‡ì—ê²Œ ë¨¼ì € ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤.")
        return False

    logger.info("\nğŸ“± í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘...")
    
    # HTML íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ í•¨ìˆ˜
    def escape_html(text: str) -> str:
        """HTML ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        return (text.replace('&', '&amp;')
                   .replace('<', '&lt;')
                   .replace('>', '&gt;'))

    def escape_url(url: str) -> str:
        """URLì„ HTML ì†ì„±ì— ì•ˆì „í•˜ê²Œ ì‚½ì…í•  ìˆ˜ ìˆë„ë¡ ì´ìŠ¤ì¼€ì´í”„í•©ë‹ˆë‹¤."""
        if not url:
            return ""
        return escape_html(url).replace('"', '&quot;')
    
    # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ êµ¬ì„± (4096ì ì œí•œ ê³ ë ¤, ë¶„í•  ë¡œì§ í†µì¼)
    max_length = 4096
    messages = []
    current_message = f"ğŸ›¡ï¸ <b>{TODAY_STR} ë³´ì•ˆ ë¸Œë¦¬í•‘</b>\n\n"

    for i, item in enumerate(articles, 1):
        title = escape_html(item.get('title', ''))
        safe_url = escape_url(item.get('url', ''))
        display_url = escape_html(item.get('url', ''))
        category = escape_html(item.get('category', ''))

        new_line = f"{i}. {category} <b>{title}</b>\n"

        # í•´ì™¸ ê¸°ì‚¬ ì›ë¬¸ í‘œì‹œ
        if '[í•´ì™¸]' in item.get('category', '') and 'title_original' in item and item['title_original']:
            title_original = escape_html(item['title_original'])
            new_line += f"   ğŸŒ <i>{title_original}</i>\n"

        new_line += f"   ğŸ”— <a href=\"{safe_url}\">{display_url}</a>\n\n"

        if len(current_message) + len(new_line) > max_length - 50:
            messages.append(current_message + "<i>ê³„ì†...</i>")
            current_message = f"ğŸ›¡ï¸ <b>{TODAY_STR} ë³´ì•ˆ ë¸Œë¦¬í•‘ (ê³„ì†)</b>\n\n"

        current_message += new_line

    current_message += "<i>ë.</i>"
    messages.append(current_message)
    
    # í…”ë ˆê·¸ë¨ APIë¡œ ë©”ì‹œì§€ ì „ì†¡
    telegram_api_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    success_count = 0
    for msg in messages:
        try:
            data = {
                "chat_id": chat_id,
                "text": msg,
                "parse_mode": "HTML",
                "disable_web_page_preview": False
            }

            res = requests.post(telegram_api_url, json=data, timeout=10)
            
            if res.status_code == 200:
                success_count += 1
                logger.info(f"   âœ… í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ {success_count}/{len(messages)} ì „ì†¡ ì™„ë£Œ")
            else:
                error_response = res.json() if res.text else {}
                error_description = error_response.get('description', res.text[:200])
                error_code = error_response.get('error_code', res.status_code)
                
                logger.error(f"   âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {error_code} - {error_description}")
                
                # ìì„¸í•œ ì˜¤ë¥˜ ì•ˆë‚´
                if "chat not found" in error_description.lower():
                    logger.error("   ğŸ’¡ í•´ê²° ë°©ë²•:")
                    logger.error("      1. ê°œì¸ ì±„íŒ…: ë´‡ì—ê²Œ ë¨¼ì € ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„¸ìš” (/start)")
                    logger.error("      2. ê·¸ë£¹ ì±„íŒ…: ë´‡ì„ ê·¸ë£¹ì— ì¶”ê°€í•˜ê³  ê´€ë¦¬ì ê¶Œí•œì„ ë¶€ì—¬í•˜ì„¸ìš”")
                    logger.error("      3. ì±„íŒ… ID í™•ì¸: @userinfobotì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„œ IDë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    logger.error(f"      4. í˜„ì¬ ì±„íŒ… ID: {chat_id}")
                elif "unauthorized" in error_description.lower():
                    logger.error("   ğŸ’¡ ë´‡ í† í°ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. GitHub Secretsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                return False
                
            # ë©”ì‹œì§€ ê°„ ì§§ì€ ëŒ€ê¸° (API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€)
            if len(messages) > 1:
                time.sleep(1)
                
        except requests.exceptions.RequestException as e:
            logger.error(f"   âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return False
        except Exception as e:
            logger.error(f"   âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    if success_count == len(messages):
        logger.info("âœ… í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")
        return True
    else:
        logger.warning(f"âš ï¸ í…”ë ˆê·¸ë¨ ì „ì†¡ ë¶€ë¶„ ì‹¤íŒ¨ ({success_count}/{len(messages)})")
        return False


# ==========================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    global NOW, TODAY_STR, YESTERDAY
    NOW = datetime.now(KST)
    TODAY_STR = NOW.strftime("%Y-%m-%d")
    YESTERDAY = (NOW - timedelta(days=1)).strftime("%Y-%m-%d")
    logger.info(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ(KST): {TODAY_STR} (ì–´ì œ: {YESTERDAY} ì´í›„ ê¸°ì‚¬ë§Œ í—ˆìš©)")

    try:
        logger.info("=" * 50)
        logger.info("ê¸ˆìœµê¶Œ ë³´ì•ˆ ë‰´ìŠ¤ ë´‡ ì‹œì‘")
        logger.info("=" * 50)
        
        final_news = process_news()

        if final_news:
            logger.info(f"\nğŸ“Š ìµœì¢… ì„ ë³„ëœ ë‰´ìŠ¤: {len(final_news)}ê°œ")

            # í…”ë ˆê·¸ë¨ ì „ì†¡ (ê¸°ì¡´)
            send_telegram(final_news)

            # ì›¹ì‚¬ì´íŠ¸ìš© ì²˜ë¦¬ (ì‹ ê·œ) - í…”ë ˆê·¸ë¨ ì‹¤íŒ¨ì™€ ë…ë¦½
            try:
                # ì „ëµì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (GPT-4o)
                analysis = generate_strategic_analysis(final_news)

                # SQLite ì €ì¥
                save_to_sqlite(final_news, analysis, TODAY_STR)
            except Exception as e:
                logger.error(f"âŒ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨ (í…”ë ˆê·¸ë¨ ì „ì†¡ì—ëŠ” ì˜í–¥ ì—†ìŒ): {e}")
        else:
            logger.warning("âš ï¸ ìµœì¢… ì„ ë³„ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        logger.info("=" * 50)
        logger.info("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
